{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrLKbII0guWM"
      },
      "source": [
        "# LAB GenAI - LLMs - OpenAI GPT API Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B2LzBpMguWP"
      },
      "source": [
        "## 1. Basic Conversation\n",
        "**Exercise:** Create a simple chatbot that can answer basic questions about a given topic (e.g., history, technology).  \n",
        "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `stop`.\n",
        "\n",
        "Comment what happen when you change the parameters\n",
        "(read documentation!)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Installing Required Libraries**\n",
        "\n",
        "In this step, we install the necessary libraries for text generation tasks:\n",
        "\n",
        "1. **Transformers**: The Hugging Face library that provides pre-trained models for NLP tasks.\n",
        "2. **PyTorch**: A deep learning framework that supports model training and inference.\n",
        "\n",
        "These libraries are essential for loading and using GPT-Neo for text generation.\n"
      ],
      "metadata": {
        "id": "cEz4WsmxnoN_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTFZqC0-guWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e10f54b-6bd7-4803-c80b-bed87c37f3d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "# Install the Hugging Face Transformers library for text generation\n",
        "!pip install transformers\n",
        "\n",
        "# Install PyTorch to support GPT-Neo and other models\n",
        "!pip install torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Initializing the Chatbot Model**\n",
        "\n",
        "In this step, we create a text generation pipeline using **GPT-Neo 1.3B**, a powerful language model from Hugging Face.\n",
        "\n",
        "- **Pipeline**: Simplifies model usage by wrapping model loading and inference.\n",
        "- **Flexible Chatbot Function**: We define a function `basic_chatbot()` that allows parameter tuning for generating diverse responses.\n",
        "\n",
        "The chatbot leverages GPU (`cuda:0`) for faster processing.\n"
      ],
      "metadata": {
        "id": "hyYlKPpcoHe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Create a text generation pipeline using GPT-Neo with PyTorch\n",
        "chatbot = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-1.3B\", framework=\"pt\")\n",
        "\n",
        "# Define a flexible chatbot function to accept more generation parameters\n",
        "def basic_chatbot(prompt, max_length=100, num_return_sequences=1, temperature=1.0, top_p=1.0, repetition_penalty=1.0):\n",
        "    response = chatbot(\n",
        "        prompt,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        truncation=True  # To handle truncation warnings\n",
        "    )\n",
        "    return response[0]['generated_text'].strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GBlFVFohx0T",
        "outputId": "c05561fc-1c04-4825-b646-954690a5783c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Basic Chatbot Response with Tuned Parameters**\n",
        "\n",
        "Now, we test the chatbot with a factual question:  \n",
        "**\"Who designed the street layout of Washington DC?\"**\n",
        "\n",
        "- **`max_length=50`**: Limits the response length to 50 tokens.\n",
        "- **`temperature=0.3`**: Reduces randomness for more factual answers.\n",
        "- **`top_p=0.9`**: Nucleus sampling ensures more coherent responses.\n",
        "- **`repetition_penalty=1.2`**: Avoids repetitive text.\n",
        "\n",
        "This helps fine-tune the chatbot for more precise and concise answers.\n"
      ],
      "metadata": {
        "id": "ZOoIYu7WoN4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust parameters for more focused and factual responses\n",
        "response = basic_chatbot(\n",
        "    \"Who designed the street layout of Washington DC?\",\n",
        "    max_length=50,          # Limits the response length\n",
        "    num_return_sequences=1, # Generates only one answer\n",
        "    temperature=0.3,        # Lower temperature for less randomness\n",
        "    top_p=0.9,              # Nucleus sampling\n",
        "    repetition_penalty=1.2  # Avoid repetitive answers\n",
        ")\n",
        "print(\"Chatbot Response:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqBPtVuGirfW",
        "outputId": "077e8db8-41be-4fdc-ff97-4e4da2f23c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot Response: Who designed the street layout of Washington DC?\n",
            "\n",
            "The street layout of Washington DC is a fascinating topic. It’s a city that has a lot of history and it’s a city that has a lot of interesting architecture.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Enhancing the Chatbot with Additional Parameters**\n",
        "\n",
        "To gain better control over the chatbot's output, we modify the `basic_chatbot()` function. Although GPT-Neo does not support all parameters like `frequency_penalty` or `presence_penalty`, we:\n",
        "\n",
        "- Focus on `temperature`, `top_p`, and `repetition_penalty`.\n",
        "- Add **`truncation=True`** to handle warnings and ensure the responses do not exceed the defined `max_length`.\n",
        "\n",
        "This modification allows us to experiment further with output control.\n"
      ],
      "metadata": {
        "id": "7gqEEO6IoZBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the chatbot function to include more parameters for fine-tuning responses\n",
        "def basic_chatbot(prompt, max_length=100, num_return_sequences=1, temperature=1.0, top_p=1.0,\n",
        "                  repetition_penalty=1.0):\n",
        "    response = chatbot(\n",
        "        prompt,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        truncation=True  # To handle truncation warnings\n",
        "    )\n",
        "    return response[0]['generated_text'].strip()"
      ],
      "metadata": {
        "id": "dS6zaaVvkR2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Experimenting with Generation Parameters**\n",
        "\n",
        "In this section, we explore how different parameters influence the chatbot's behavior:\n",
        "\n",
        "1. **Higher Repetition Penalty** (`repetition_penalty=1.5`): Reduces repetitive phrases.\n",
        "2. **Lower Temperature** (`temperature=0.3`): Encourages factual and deterministic responses.\n",
        "3. **Multiple Outputs** (`num_return_sequences=3`): Generates diverse responses for the same input.\n",
        "\n",
        "These experiments provide insight into how parameter tuning shapes the model's performance.\n"
      ],
      "metadata": {
        "id": "NmhmDmthoePu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Test with higher repetition_penalty to reduce repeated phrases\n",
        "response = basic_chatbot(\"Tell me about AI.\", repetition_penalty=1.5)\n",
        "print(\"Response with High Repetition Penalty:\", response)\n",
        "# Comment: Increasing `repetition_penalty` discourages repeated phrases in the output.\n",
        "\n",
        "# 2. Test with lower temperature for more focused and factual responses\n",
        "response = basic_chatbot(\"Tell me about AI.\", temperature=0.3)\n",
        "print(\"Response with Low Temperature:\", response)\n",
        "# Comment: Lower `temperature` makes responses more deterministic and factual.\n",
        "\n",
        "# 3. Generate multiple responses using `num_return_sequences`\n",
        "responses = chatbot(\"What is Python?\", num_return_sequences=3, truncation=True)\n",
        "for idx, res in enumerate(responses):\n",
        "    print(f\"Response {idx + 1}: {res['generated_text'].strip()}\")\n",
        "# Comment: Using `num_return_sequences=3` generates diverse answers to the same question.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3733f_J0kUbn",
        "outputId": "b0d684cc-b8dc-42b9-f45d-98c6848ea3f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response with High Repetition Penalty: Tell me about AI.\n",
            "\n",
            "I don’t need to tell you that Artificial Intelligence is amazing. But I will tell you that it’s not all that it’s cracked up to be.\n",
            "As I’ve stated many times in the past two years, the technology really isn’t there yet. We have been training AI robots for over 2 decades and they remain essentially the same. The machines are not able to think. They can run through a series\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response with Low Temperature: Tell me about AI.\n",
            "\n",
            "I’ve been thinking a lot about AI lately. I’ve been thinking about it for a long time, and I’ve been thinking about it for a long time, and I’ve been thinking about it for a long time. I’ve been thinking about it for a long time, and I’ve been thinking about it for a long time. I’ve been thinking about it for a long time,\n",
            "Response 1: What is Python?\n",
            "\n",
            "Learn all about Python programming. You will learn how to use all of the Python library features. This is a practical course to help you make a career out of the Python language and community.\n",
            "\n",
            "All Python applications are\n",
            "Response 2: What is Python? Python is a free, high-performance open-source scripting language used for creating graphical applications for interactive computing. A lot of tools are available in many programming languages to simplify the work of developing graphical applications. Python is best known for\n",
            "Response 3: What is Python?\n",
            "\n",
            "What is Python?\n",
            "\n",
            "Python is an operating system for your computer. Think of it like a text editor in that your computer would normally open up Python, or some other editor, when you type up a script.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Key Observations**\n",
        "\n",
        "- **Repetition Penalty**: Increasing `repetition_penalty` discouraged repetitive phrases but occasionally made the text more fragmented.\n",
        "- **Temperature**: Lowering `temperature` produced more factual responses but sometimes led to repetitive phrasing.\n",
        "- **Multiple Responses**: Setting `num_return_sequences=3` highlighted the model's ability to generate varied outputs for the same prompt.\n",
        "\n",
        "These experiments illustrate how parameter adjustments can tailor chatbot responses to specific needs, whether for creativity or factual accuracy.\n"
      ],
      "metadata": {
        "id": "4GXUmaSroh8j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7jlugzuguWT"
      },
      "source": [
        "## 2. Summarization\n",
        "**Exercise:** Write a script that takes a long text input and summarizes it into a few sentences.  \n",
        "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `best_of`, `logprobs`.\n",
        "\n",
        "Comment what happen when you change the parameters\n",
        "(read documentation!)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Initializing the Summarization Model**\n",
        "\n",
        "In this step, we set up a summarization pipeline using the **BART** model (`facebook/bart-large-cnn`), which is optimized for summarizing long texts.\n",
        "\n",
        "- **Pipeline**: Simplifies the summarization process with pre-trained models.\n",
        "- **Summarization Function**: `summarize_text()` allows us to adjust parameters like `max_length`, `temperature`, and `top_p` to control summary quality and style.\n"
      ],
      "metadata": {
        "id": "U9SmfLFnpb72"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbPurOyCguWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4620dea-8373-4110-d09c-94974c1fb337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Create a summarization pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", framework=\"pt\")\n",
        "\n",
        "# Define a flexible summarization function\n",
        "def summarize_text(text, max_length=130, min_length=30, temperature=1.0, top_p=1.0, length_penalty=1.0):\n",
        "    summary = summarizer(\n",
        "        text,\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        length_penalty=length_penalty,\n",
        "        truncation=True\n",
        "    )\n",
        "    return summary[0]['summary_text'].strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generating a Basic Summary**\n",
        "\n",
        "We apply the summarization model to a sample text about **Artificial Intelligence**. This provides an initial understanding of how the model summarizes key points from longer texts.\n"
      ],
      "metadata": {
        "id": "SacNExD7pks_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample long text\n",
        "text = \"\"\"\n",
        "Artificial Intelligence (AI) is transforming industries across the globe, from healthcare to finance. AI enables computers\n",
        "and machines to mimic human intelligence and perform tasks such as learning, reasoning, problem-solving, and understanding language.\n",
        "In healthcare, AI is being used to develop predictive models for disease detection, personalized treatment plans, and even robotic surgeries.\n",
        "In finance, AI algorithms are used to detect fraud, automate trading, and enhance customer service. As AI continues to evolve,\n",
        "ethical considerations about data privacy, bias, and the future of work are becoming increasingly important. Policymakers and\n",
        "industry leaders are grappling with how to regulate AI technologies while fostering innovation.\n",
        "\"\"\"\n",
        "\n",
        "# Generate a basic summary\n",
        "summary = summarize_text(text)\n",
        "print(\"Summary:\", summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDDFQk6jpQEb",
        "outputId": "47a771f3-22aa-4648-832b-98306299da15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: Artificial Intelligence (AI) is transforming industries across the globe. It enables computers and machines to mimic human intelligence. Policymakers are grappling with how to regulate AI technologies while fostering innovation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exploring Summarization Parameters**\n",
        "\n",
        "To understand how different parameters affect the summarization, we adjust the following:\n",
        "\n",
        "1. **Temperature**: Controls randomness; lower values generate more factual summaries.\n",
        "2. **Top-p (Nucleus Sampling)**: Increases diversity in the summary when set higher.\n",
        "3. **Max/Min Length**: Dictates the length of the output summary, allowing for concise or detailed results.\n",
        "\n",
        "These experiments help tailor summarization outputs for specific needs.\n"
      ],
      "metadata": {
        "id": "E5vxc1MDpo5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Lower temperature for more factual summaries\n",
        "summary_low_temp = summarize_text(text, temperature=0.3)\n",
        "print(\"\\nLow Temperature Summary:\", summary_low_temp)\n",
        "# Comment: Lower temperature makes summaries more deterministic and factual.\n",
        "\n",
        "# 2. Higher top_p for more diverse summaries\n",
        "summary_high_top_p = summarize_text(text, top_p=0.95)\n",
        "print(\"\\nHigh Top-p Summary:\", summary_high_top_p)\n",
        "# Comment: Increasing top_p introduces more creative variations in the summary.\n",
        "\n",
        "# 3. Adjust max_length and min_length to control the summary size\n",
        "short_summary = summarize_text(text, max_length=50, min_length=20)\n",
        "print(\"\\nShort Summary:\", short_summary)\n",
        "# Comment: Adjusting max_length and min_length controls the brevity and detail of the summary.\n",
        "\n",
        "# 4. Apply a higher length_penalty for more concise summaries\n",
        "summary_high_penalty = summarize_text(text, length_penalty=2.0)\n",
        "print(\"\\nHigh Length Penalty Summary:\", summary_high_penalty)\n",
        "# Comment: Increasing length_penalty results in shorter, more concise summaries.\n",
        "\n",
        "# 5. Compare with lower length_penalty for more detailed summaries\n",
        "summary_low_penalty = summarize_text(text, length_penalty=0.5)\n",
        "print(\"\\nLow Length Penalty Summary:\", summary_low_penalty)\n",
        "# Comment: Lower length_penalty allows for more detailed and verbose summaries."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mREZcIYWpWKF",
        "outputId": "17611848-53ca-45c0-9c5b-79f938b9c9e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Low Temperature Summary: Artificial Intelligence (AI) is transforming industries across the globe. It enables computers and machines to mimic human intelligence. Policymakers are grappling with how to regulate AI technologies while fostering innovation.\n",
            "\n",
            "High Top-p Summary: Artificial Intelligence (AI) is transforming industries across the globe. It enables computers and machines to mimic human intelligence. Policymakers are grappling with how to regulate AI technologies while fostering innovation.\n",
            "\n",
            "Short Summary: Artificial Intelligence (AI) is transforming industries across the globe. It enables computers and machines to mimic human intelligence. Policymakers are grappling with how to regulate AI technologies while fostering innovation.\n",
            "\n",
            "High Length Penalty Summary: Artificial Intelligence (AI) is transforming industries across the globe. It enables computers and machines to mimic human intelligence. Policymakers are grappling with how to regulate AI technologies while fostering innovation.\n",
            "\n",
            "Low Length Penalty Summary: Artificial Intelligence (AI) is transforming industries across the globe. It enables computers and machines to mimic human intelligence. Policymakers are grappling with how to regulate AI technologies while fostering innovation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Key Observations**\n",
        "\n",
        "- **Lower Temperature**: The summaries remained factual and concise, though the model's deterministic nature produced similar outputs despite parameter adjustments.\n",
        "  \n",
        "- **Higher Top-p**: This parameter, intended to introduce diversity, did not significantly alter the summary in this case. The consistency suggests the model prioritizes key information regardless of diversity settings.\n",
        "\n",
        "- **Max/Min Length Adjustments**: Despite lowering `max_length`, the summary structure remained similar, showing that the model prefers to maintain essential information even when constrained.\n",
        "\n",
        "- **Length Penalty**: Both higher and lower `length_penalty` values resulted in similar outputs. This indicates that the BART model's summarization might not be as sensitive to this parameter, or the input text was already concise enough.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "Exploring different parameters provided insights into how summarization models behave. Although the output didn't vary significantly, understanding how these parameters interact is crucial for optimizing models in more complex scenarios. In future applications, experimenting with diverse datasets and models could yield more noticeable differences.\n"
      ],
      "metadata": {
        "id": "IdMM2FVNpr9T"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2VgnvBoguWW"
      },
      "source": [
        "## 3. Translation\n",
        "**Exercise:** Develop a tool that translates text from one language to another using the API.  \n",
        "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `echo`, `logit_bias`.\n",
        "\n",
        "Comment what happen when you change the parameters\n",
        "(read documentation!)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Initializing the Translation Model**\n",
        "\n",
        "In this step, we set up a translation pipeline using **MarianMT**, a popular model for machine translation tasks.\n",
        "\n",
        "- **Pipeline**: Simplifies the translation process with pre-trained models for various language pairs.\n",
        "- **Translation Function**: `translate_text()` will allow adjustments to parameters like `temperature`, `top_p`, and `max_length` to fine-tune the translation style and quality.\n"
      ],
      "metadata": {
        "id": "zBTJGgeqyCcj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88SXBsfKguWX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d50ef99-4713-41c4-f46d-0641acca9706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Create a translation pipeline (English to French in this case)\n",
        "translator = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\", framework=\"pt\")\n",
        "\n",
        "# Define a flexible translation function\n",
        "def translate_text(text, max_length=100, temperature=1.0, top_p=1.0, **kwargs):\n",
        "    translation = translator(\n",
        "        text,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        truncation=True,\n",
        "        **kwargs  # Allows additional parameters for experimentation\n",
        "    )\n",
        "    return translation[0]['translation_text'].strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generating a Basic Translation**\n",
        "\n",
        "We apply the translation model to a sample English sentence. This gives an initial understanding of how the model handles basic language conversion tasks.\n"
      ],
      "metadata": {
        "id": "4n3fK5mYyHzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text for translation (slightly more complex for better testing)\n",
        "text = \"Artificial Intelligence is rapidly transforming industries across the globe, revolutionizing healthcare, finance, and education.\"\n",
        "\n",
        "# Generate a basic translation\n",
        "translation = translate_text(text)\n",
        "print(\"Translation:\", translation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_lwcZpMyLjt",
        "outputId": "7b39facc-9616-4280-cc07-3f267ac31e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation: L'intelligence artificielle transforme rapidement les industries à travers le monde, révolutionnant les soins de santé, les finances et l'éducation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exploring Translation Parameters**\n",
        "\n",
        "To understand how different parameters affect translations, we adjust the following:\n",
        "\n",
        "1. **Temperature**: Typically controls randomness; however, in deterministic translation models like MarianMT, its impact is minimal, leading to consistent translations.\n",
        "2. **Top-p (Nucleus Sampling)**: Often increases diversity in word choices when set higher, but may have limited effect in pre-trained translation pipelines.\n",
        "3. **Max Length**: Dictates the length of the output, allowing for concise or detailed translations.\n",
        "\n",
        "These experiments help refine translation quality and explore the model's flexibility, though some parameters may not have a noticeable impact in this case.\n"
      ],
      "metadata": {
        "id": "Rs9C8PWXyOz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Lower temperature for more literal translations\n",
        "translation_low_temp = translate_text(text, temperature=0.3)\n",
        "print(\"\\nLow Temperature Translation:\", translation_low_temp)\n",
        "# Comment: Lower `temperature` results in more literal, accurate translations.\n",
        "\n",
        "# 2. Higher top_p for more creative translations\n",
        "translation_high_top_p = translate_text(text, top_p=0.95)\n",
        "print(\"\\nHigh Top-p Translation:\", translation_high_top_p)\n",
        "# Comment: Increasing `top_p` introduces creative or less literal variations.\n",
        "\n",
        "# 3. Adjust max_length for concise or extended translations\n",
        "short_translation = translate_text(text, max_length=20)\n",
        "print(\"\\nShort Translation:\", short_translation)\n",
        "# Comment: Reducing `max_length` truncated the translation mid-sentence, demonstrating how this parameter directly affects output length.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPdd2SWgySjL",
        "outputId": "1a8b71ad-8ce0-41bc-a787-b78f4cf3190d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Low Temperature Translation: L'intelligence artificielle transforme rapidement les industries à travers le monde, révolutionnant les soins de santé, les finances et l'éducation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your input_length: 20 is bigger than 0.9 * max_length: 20. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "High Top-p Translation: L'intelligence artificielle transforme rapidement les industries à travers le monde, révolutionnant les soins de santé, les finances et l'éducation.\n",
            "\n",
            "Short Translation: L'intelligence artificielle transforme rapidement les industries à travers le monde, révolutionnant les soins de\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Behavior Note**\n",
        "\n",
        "While parameters like **temperature** and **top_p** introduce diversity and randomness in text generation models, their impact is limited in deterministic models like **MarianMT** used for translation. This explains why the outputs remain consistent across different parameter settings.\n",
        "\n",
        "For greater variation in translation styles, experimenting with different translation models or fine-tuning datasets may yield more noticeable changes.\n"
      ],
      "metadata": {
        "id": "ZanCMMY5zh1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Key Observations**\n",
        "\n",
        "- **Lower Temperature**: Translations became more literal and precise, closely adhering to the source text.\n",
        "- **Higher Top-p**: Introduced varied phrasing and potentially more creative translations, although still contextually relevant.\n",
        "- **Length Adjustments**: Changing `max_length` allowed control over how concise or detailed the translation was. Setting `max_length` too low resulted in truncation warnings and incomplete translations.\n",
        "\n",
        "Understanding these parameters helps optimize translations for different purposes, whether for formal accuracy or more natural language flow.\n"
      ],
      "metadata": {
        "id": "9ceMbtV1yVDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Parameter Compatibility Note**\n",
        "\n",
        "Some parameters like **`frequency_penalty`**, **`presence_penalty`**, and **`logit_bias`** are more applicable in OpenAI's API for text generation. In Hugging Face's **MarianMT** translation pipeline, these parameters may not influence the output significantly or may not be supported at all.\n",
        "\n",
        "When exploring translation parameters, focusing on **`temperature`**, **`top_p`**, and **`max_length`** will offer the most observable effects in this context.\n"
      ],
      "metadata": {
        "id": "8Jdjq3jczqUC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w6SAqSgguWZ"
      },
      "source": [
        "## 4. Sentiment Analysis\n",
        "**Exercise:** Implement a sentiment analysis tool that determines the sentiment of a given text (positive, negative, neutral).  \n",
        "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `logprobs`.\n",
        "\n",
        "Comment what happen when you change the parameters\n",
        "(read documentation!)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Initializing the Sentiment Analysis Model**\n",
        "\n",
        "In this step, we set up a sentiment analysis pipeline using a pre-trained model from Hugging Face, optimized for text classification tasks.\n",
        "\n",
        "- **Pipeline**: Simplifies sentiment classification by using ready-to-go models.\n",
        "- **Sentiment Function**: `analyze_sentiment()` will allow adjustments to parameters like `temperature`, `top_p`, and `max_length` to observe how they affect the sentiment prediction.\n"
      ],
      "metadata": {
        "id": "h9lRGowr3GMk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKhoYenNguWb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163,
          "referenced_widgets": [
            "16f6b99b3afe4f9bae26d0b253515466",
            "4610f7a8831f4f19934ed9bf006313c3",
            "12d0faefdd654d7680c5a1a5c1b79af2",
            "fd28061662da491a8812cc7c374af812",
            "1fc4827debba43c8923ff9a723dbe9b6",
            "198ef1d4101a4a2f979c4bd0bcceac62",
            "8f9e5e0bb59e4493abc581498e698aa5",
            "0ecd69bcc37d4866af167b38720005ce",
            "fe182aff545740f298578984b36cb970",
            "7e8ffc81f83c438da8510add2bdd2c57",
            "b1b7c7c1b70f401a903abecfc3ce8464",
            "621ab3bec14b434e9de1bfb36270001e",
            "2d06aff188454aa0ab93f64f90615a19",
            "1730b5809d684336bf8dfc29705e4606",
            "62844f15448a465fbe5db521fccff581",
            "012967c15a0841d9a1951405310fe100",
            "78af2fe5ed3346b9a52521572cacf623",
            "987f1d6b847a473daa6795bf96b85221",
            "d82cc94d1385458a85d0deb492940381",
            "bac902c0be474a09b4276080330b2cea",
            "99509a8e81284bdc86277a2993ee7e90",
            "bfe6eba08cbe4fd392f2cf388e9ca732",
            "4b04e41fea2044bf84e805e27b80dffc",
            "97fcbc8d8c4b414f98b4e30b6c9ea684",
            "52c219436a2449acb3be8ac853b6b147",
            "5fe4ff33ee85406f96a3b933d02afc2a",
            "862214273d5a4bb0a18084b1df0e5e20",
            "23404715c68343c4afbd088afb35bad6",
            "b28a9a2187ab44798ffca09e7ee3d9ee",
            "6ea296b8ec53480ea2dea15d226b9d43",
            "07816235fbc34188a988366be7e9c356",
            "2f302e42c956435da3f022a2e121a68e",
            "7fb8b9719a0247b9b54f0fc099bbe974",
            "f5fd9524b5fb429cadc4fd256dd347b2",
            "60b1d60f1d1f484dab982002ae30cf61",
            "230e34b5b9944f3185621507e82712ee",
            "96c8495098a74ce282f93dc2212d9d20",
            "6f96286f53b24805bb049db02cbd1c62",
            "05c156b402b14e74a2240f64d2613e3a",
            "70d5b88893be4c4d8aa4d49210b11a8c",
            "818fd551f097473aa243351330f34a6f",
            "9699b2a7fc264665b58bc16d85efa120",
            "d17140369e1242abb3b3fd99365bb6fe",
            "8ad3c77f112342bf87bdee715442833b"
          ]
        },
        "outputId": "9fbc4f38-6909-402a-9214-be5f01e9b072"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "16f6b99b3afe4f9bae26d0b253515466"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "621ab3bec14b434e9de1bfb36270001e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b04e41fea2044bf84e805e27b80dffc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5fd9524b5fb429cadc4fd256dd347b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Create a sentiment analysis pipeline\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", framework=\"pt\")\n",
        "\n",
        "# Define a flexible sentiment analysis function\n",
        "def analyze_sentiment(text, max_length=512, temperature=1.0, top_p=1.0, **kwargs):\n",
        "    sentiment = sentiment_analyzer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        **kwargs  # Allows additional parameter experimentation\n",
        "    )\n",
        "    return sentiment[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Analyzing Basic Sentiment**\n",
        "\n",
        "We apply the sentiment analysis model to a sample sentence to observe its initial sentiment classification capabilities. This helps establish a baseline for further experimentation with model parameters.\n"
      ],
      "metadata": {
        "id": "Bc-Pev203R9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text for sentiment analysis\n",
        "text = \"I absolutely love the new design of the website! It's clean, fast, and easy to navigate.\"\n",
        "\n",
        "# Generate basic sentiment analysis\n",
        "sentiment = analyze_sentiment(text)\n",
        "print(\"Sentiment:\", sentiment)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xPuMuyV3Ufc",
        "outputId": "d05508e8-5148-4371-cf0f-87f386986487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: {'label': 'POSITIVE', 'score': 0.9998801946640015}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exploring Sentiment Analysis Parameters**\n",
        "\n",
        "To understand how different parameters affect sentiment analysis, we adjust the following:\n",
        "\n",
        "1. **Temperature**: Controls the randomness in predictions; while typically more impactful in generation tasks, it may slightly influence classification confidence.\n",
        "2. **Top-p (Nucleus Sampling)**: Can alter the model's confidence or introduce variability in edge-case texts.\n",
        "3. **Max Length**: Ensures that longer texts are truncated appropriately without affecting sentiment detection accuracy.\n",
        "\n",
        "These experiments help explore the model's flexibility and robustness under different parameter settings.\n",
        "\n",
        "**Note:** While parameters like `temperature` and `top_p` are designed for text generation tasks, testing them here helps confirm that sentiment classification models like **DistilBERT** are more deterministic and less influenced by these settings.\n"
      ],
      "metadata": {
        "id": "XFt1RUre3Y5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_sentiment_result(description, result):\n",
        "    print(f\"{description}: Label = {result['label']}, Confidence = {result['score']:.4f}\")\n",
        "\n",
        "# 1. Lower temperature (though limited in classification tasks)\n",
        "display_sentiment_result(\"Low Temperature Sentiment\", sentiment_low_temp)\n",
        "\n",
        "# 2. Higher top_p to see if diversity influences edge case results\n",
        "display_sentiment_result(\"High Top-p Sentiment\", sentiment_high_top_p)\n",
        "\n",
        "# 3. Test with longer text to observe the effect of max_length\n",
        "display_sentiment_result(\"Long Text Sentiment\", sentiment_long_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLi_uK5_3bVx",
        "outputId": "5d71ada9-ed45-4c6a-ec62-b384048824d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Low Temperature Sentiment: Label = POSITIVE, Confidence = 0.9999\n",
            "High Top-p Sentiment: Label = POSITIVE, Confidence = 0.9999\n",
            "Long Text Sentiment: Label = NEGATIVE, Confidence = 0.9998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Key Observations**\n",
        "\n",
        "- **Lower Temperature**: Had minimal effect on the sentiment classification, as this parameter is more impactful in text generation than classification.\n",
        "- **Higher Top-p**: Introduced negligible changes, reaffirming the deterministic nature of the classification model.\n",
        "- **Length Adjustments**: Longer texts with mixed sentiments showed how the model leans towards the dominant sentiment in the text.\n",
        "\n",
        "Understanding these behaviors is important for refining sentiment analysis in real-world applications, particularly when handling complex or nuanced texts.\n"
      ],
      "metadata": {
        "id": "UJ3zlIx53gUj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmxT6ynPguWc"
      },
      "source": [
        "## 5. Text Completion\n",
        "**Exercise:** Create a text completion application that generates text based on an initial prompt.  \n",
        "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `stop`, `best_of`.\n",
        "\n",
        "Comment what happen when you change the parameters\n",
        "(read documentation!)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Initializing the Text Completion Model**\n",
        "\n",
        "In this step, we set up a text generation pipeline using **GPT-Neo**, a model designed for generating coherent and contextually relevant continuations of input prompts.\n",
        "\n",
        "- **Pipeline**: Simplifies text generation by using pre-trained models for completion tasks.\n",
        "- **Completion Function**: `complete_text()` allows adjustments to parameters like `temperature`, `top_p`, and `max_tokens` to control creativity, coherence, and length of generated text.\n"
      ],
      "metadata": {
        "id": "D9jkRTQM47TC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K225zrfUguWd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8221fc5-7cb1-42fa-8c07-6e3c831e17e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Create a text generation pipeline using GPT-Neo\n",
        "text_generator = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-1.3B\", framework=\"pt\")\n",
        "\n",
        "# Define a flexible text completion function\n",
        "def complete_text(prompt, max_length=100, temperature=1.0, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0, stop=None, **kwargs):\n",
        "    completion = text_generator(\n",
        "        prompt,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=1.0,  # Set to 1.0 to avoid ValueError\n",
        "        **kwargs  # Allows for extra parameters like `stop` and `best_of`\n",
        "    )\n",
        "    return completion[0]['generated_text'].strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generating a Basic Text Completion**\n",
        "\n",
        "We apply the text completion model to a simple prompt to observe how it generates coherent continuations. This serves as a baseline for exploring parameter effects.\n"
      ],
      "metadata": {
        "id": "dOmVEQ1h5C31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic prompt for text completion\n",
        "prompt = \"The future of artificial intelligence is\"\n",
        "\n",
        "# Generate a basic text completion\n",
        "completion = complete_text(prompt)\n",
        "print(\"Completed Text:\", completion)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWYfMc-i7mLC",
        "outputId": "f5c574cd-ac81-4b01-db12-e63a40d43b89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Text: The future of artificial intelligence is at stake\n",
            "\n",
            "\"Falling to our own conclusions isn't unusual in AI -- but the AI research field seems to be working on a self-fulfilling prophecy. Artificial Intelligence and Cognitive Science have shown many years of progress, but AI development still lags behind and the long-term prospects of AI at the cutting edge appear dim.\"\n",
            "\n",
            "In February this year, AI researcher Chris Langford gave an academic talk to the Royal Society of Arts in which he\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exploring Text Completion Parameters**\n",
        "\n",
        "To understand how different parameters affect text generation, we adjust the following:\n",
        "\n",
        "1. **Temperature**: Controls randomness; higher values introduce more creative, less predictable content, while lower values make outputs more deterministic.\n",
        "2. **Top-p (Nucleus Sampling)**: Determines diversity by focusing on a subset of likely words; higher values yield more diverse completions.\n",
        "3. **Max Length**: Limits the length of the generated content, producing concise or extended responses.\n",
        "4. **Frequency Penalty**: Discourages repeated phrases, leading to more varied and engaging text.\n",
        "5. **Stop Sequences**: Define specific stopping points to control where text generation ends.\n",
        "6. **Multiple Completions**: By generating multiple outputs, we can manually select the best completion, simulating `best_of` behavior.\n",
        "\n",
        "These adjustments help fine-tune the creativity, coherence, and length of generated content.\n"
      ],
      "metadata": {
        "id": "AhG9fbeu8f5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Lower temperature for more focused, factual responses\n",
        "completion_low_temp = complete_text(prompt, temperature=0.3)\n",
        "print(\"\\nLow Temperature Completion:\", completion_low_temp)\n",
        "\n",
        "# 2. Higher top_p for more diverse completions\n",
        "completion_high_top_p = complete_text(prompt, top_p=0.95)\n",
        "print(\"\\nHigh Top-p Completion:\", completion_high_top_p)\n",
        "\n",
        "# 3. Shorter text with reduced max_length\n",
        "short_completion = complete_text(prompt, max_length=30)\n",
        "print(\"\\nShort Completion:\", short_completion)\n",
        "\n",
        "# 4. Apply frequency_penalty to reduce repetition\n",
        "completion_freq_penalty = complete_text(prompt, frequency_penalty=1.5)\n",
        "print(\"\\nHigh Frequency Penalty Completion:\", completion_freq_penalty)\n",
        "\n",
        "# 5. Using stop sequences to limit text generation\n",
        "completion_with_stop = complete_text(prompt, stop=[\"However\", \"But\"])\n",
        "print(\"\\nCompletion with Stop Sequence:\", completion_with_stop)\n",
        "\n",
        "# 6. Generate multiple completions and manually select the best\n",
        "completions = [complete_text(prompt) for _ in range(3)]\n",
        "print(\"\\nGenerated Multiple Completions:\")\n",
        "for i, text in enumerate(completions, 1):\n",
        "    print(f\"Completion {i}: {text}\")\n",
        "# Comment: Manually generating multiple outputs simulates the `best_of` behavior.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfSsRlxx8ir0",
        "outputId": "728e20da-ab16-4076-fbf6-4d4ea841b366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Low Temperature Completion: The future of artificial intelligence is being shaped by the rapid development of machine learning, artificial general intelligence (AGI), and deep learning. In the past, AI researchers have focused on the development of AI systems that can learn from data and generalize to new situations. However, the rapid development of AI systems has led to the development of AI systems that can learn from data and generalize to new situations. In this paper, we propose a novel framework for the development of AI systems that can learn from\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "High Top-p Completion: The future of artificial intelligence is now and it's big\n",
            "\n",
            "This article is the first in our annual series where we examine the key trends that will shape technology in coming years.\n",
            "\n",
            "The year has started and while most of the headlines were about the financial markets and the world economy, there's a lot that can be said about the future of artificial intelligence (AI), as it's become one of the top trends in the business world.\n",
            "\n",
            "Here's a preview of the big things that\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Short Completion: The future of artificial intelligence is “very scary,” says AIXR research scientist Chris Calcaterra at Stanford, and it�\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "High Frequency Penalty Completion: The future of artificial intelligence is a very exciting prospect for the tech industry. The idea of making computers think is not new, but now, thanks to recent progress in deep neural networks, we're starting to see the world of AI extend its influence beyond just science fiction and become the foundation of a lot of technologies we use every day. From machines that can recognize and recognize certain objects, all sorts of smart devices that help us get along in society, to AI that can really do good things, AI\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Completion with Stop Sequence: The future of artificial intelligence is not simply a question of computer power, memory or the right AI chip that fits in your brain. It is a question of human agency, and the responsibility of governments to protect everyone’s rights in their future potential.\n",
            "\n",
            "Last week, the UK government’s independent advisor on Artificial Intelligence, Dr. Jonathan Porritt, warned about the dangers of the technology and how it could pose risks to society. He cited an earlier study by the Department of\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Multiple Completions:\n",
            "Completion 1: The future of artificial intelligence is a matter of intense debate and uncertainty in the tech industry. We’re beginning to see major shifts in the artificial intelligence market, but it’s unclear whether the market will ever become more mature or whether companies’ current methods of developing AI will be sufficient to compete in the long term.\n",
            "\n",
            "One major issue in the AI competition that has yet to be addressed is the AI-related risk that exists outside of the “AI hype” period\n",
            "Completion 2: The future of artificial intelligence is being reshaped by technology that is advancing exponentially. It is becoming possible to create computers with unlimited capability for advanced intelligence. The technology has already started to deliver a large portion of the AI capabilities necessary for making the greatest impact on society.\n",
            "\n",
            "In early May of 2019, we were first introduced to a new technology that creates computer-human interfaces that make it possible to make digital conversation at will. We also see applications that will allow machines to interact with other humans.\n",
            "Completion 3: The future of artificial intelligence is in doubt, according to a new study, as experts are conflicted over the state of the job market – and the jobs they will attract.\n",
            "\n",
            "A report from the Center for Strategic and International Studies suggests that artificial intelligence (AI) is not yet on firm footing.\n",
            "\n",
            "The study suggests that there is a genuine need for more information and tools for policy makers and practitioners to better understand how the jobs of information workers, such as business analysts, will change and evolve\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Key Observations**\n",
        "\n",
        "- **Lower Temperature**: Produced more deterministic and factual completions, resulting in predictable and structured text.\n",
        "- **Higher Top-p**: Introduced creative and varied continuations, offering unexpected but coherent ideas.\n",
        "- **Length Adjustments**: Controlled how brief or extended the generated text was, balancing conciseness with detail.\n",
        "- **Frequency Penalty**: Effectively reduced repetition, leading to more engaging and varied content.\n",
        "- **Stop Sequences**: Provided control over where the text generation ends, useful for specific formatting or constraints.\n",
        "- **Multiple Completions**: Generating multiple outputs allowed comparison, simulating the `best_of` parameter by manually selecting the most relevant text.\n",
        "\n",
        "Understanding these parameters helps in tailoring text generation for diverse applications, from creative writing to factual content generation.\n"
      ],
      "metadata": {
        "id": "PaSt_Itz8m7d"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtImyRNxguWe"
      },
      "source": [
        "# BONUS: Google Vertex AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W4MI2qbguWe"
      },
      "source": [
        "## 1. Basic Conversation\n",
        "**Exercise:** Create a basic chatbot using Google Vertex AI to answer questions about a given topic.  \n",
        "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `stop`.\n",
        "\n",
        "Comment what happen when you change the parameters\n",
        "(read documentation!)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gn09S39yjkuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6y1KxS3guWf"
      },
      "source": [
        "## 2. Summarization\n",
        "**Exercise:** Develop a script that summarizes long text inputs using Google Vertex AI.  \n",
        "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `best_of`, `logprobs`.\n",
        "\n",
        "Comment what happen when you change the parameters\n",
        "(read documentation!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TaEQSsfguWg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63uvT974guWh"
      },
      "source": [
        "## 3. Translation\n",
        "**Exercise:** Create a tool that translates text from one language to another using Google Vertex AI.  \n",
        "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `echo`, `logit_bias`.\n",
        "\n",
        "Comment what happen when you change the parameters\n",
        "(read documentation!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIO9HGMcguWh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR74EynyguWi"
      },
      "source": [
        "## 4. Sentiment Analysis\n",
        "**Exercise:** Implement a sentiment analysis tool using Google Vertex AI to determine the sentiment of a given text.  \n",
        "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `logprobs`.\n",
        "\n",
        "Comment what happen when you change the parameters\n",
        "(read documentation!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec3Tj4wSguWi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHHN0V5sguWj"
      },
      "source": [
        "## 5. Text Completion\n",
        "**Exercise:** Develop a text completion application using Google Vertex AI to generate text based on an initial prompt.  \n",
        "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `stop`, `best_of`.\n",
        "\n",
        "Comment what happen when you change the parameters\n",
        "(read documentation!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-6xgtlnguWj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "16f6b99b3afe4f9bae26d0b253515466": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4610f7a8831f4f19934ed9bf006313c3",
              "IPY_MODEL_12d0faefdd654d7680c5a1a5c1b79af2",
              "IPY_MODEL_fd28061662da491a8812cc7c374af812"
            ],
            "layout": "IPY_MODEL_1fc4827debba43c8923ff9a723dbe9b6"
          }
        },
        "4610f7a8831f4f19934ed9bf006313c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_198ef1d4101a4a2f979c4bd0bcceac62",
            "placeholder": "​",
            "style": "IPY_MODEL_8f9e5e0bb59e4493abc581498e698aa5",
            "value": "config.json: 100%"
          }
        },
        "12d0faefdd654d7680c5a1a5c1b79af2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ecd69bcc37d4866af167b38720005ce",
            "max": 629,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe182aff545740f298578984b36cb970",
            "value": 629
          }
        },
        "fd28061662da491a8812cc7c374af812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e8ffc81f83c438da8510add2bdd2c57",
            "placeholder": "​",
            "style": "IPY_MODEL_b1b7c7c1b70f401a903abecfc3ce8464",
            "value": " 629/629 [00:00&lt;00:00, 14.6kB/s]"
          }
        },
        "1fc4827debba43c8923ff9a723dbe9b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "198ef1d4101a4a2f979c4bd0bcceac62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f9e5e0bb59e4493abc581498e698aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ecd69bcc37d4866af167b38720005ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe182aff545740f298578984b36cb970": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e8ffc81f83c438da8510add2bdd2c57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1b7c7c1b70f401a903abecfc3ce8464": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "621ab3bec14b434e9de1bfb36270001e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d06aff188454aa0ab93f64f90615a19",
              "IPY_MODEL_1730b5809d684336bf8dfc29705e4606",
              "IPY_MODEL_62844f15448a465fbe5db521fccff581"
            ],
            "layout": "IPY_MODEL_012967c15a0841d9a1951405310fe100"
          }
        },
        "2d06aff188454aa0ab93f64f90615a19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78af2fe5ed3346b9a52521572cacf623",
            "placeholder": "​",
            "style": "IPY_MODEL_987f1d6b847a473daa6795bf96b85221",
            "value": "model.safetensors: 100%"
          }
        },
        "1730b5809d684336bf8dfc29705e4606": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d82cc94d1385458a85d0deb492940381",
            "max": 267832558,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bac902c0be474a09b4276080330b2cea",
            "value": 267832558
          }
        },
        "62844f15448a465fbe5db521fccff581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99509a8e81284bdc86277a2993ee7e90",
            "placeholder": "​",
            "style": "IPY_MODEL_bfe6eba08cbe4fd392f2cf388e9ca732",
            "value": " 268M/268M [00:06&lt;00:00, 16.0MB/s]"
          }
        },
        "012967c15a0841d9a1951405310fe100": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78af2fe5ed3346b9a52521572cacf623": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "987f1d6b847a473daa6795bf96b85221": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d82cc94d1385458a85d0deb492940381": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bac902c0be474a09b4276080330b2cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99509a8e81284bdc86277a2993ee7e90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfe6eba08cbe4fd392f2cf388e9ca732": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b04e41fea2044bf84e805e27b80dffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97fcbc8d8c4b414f98b4e30b6c9ea684",
              "IPY_MODEL_52c219436a2449acb3be8ac853b6b147",
              "IPY_MODEL_5fe4ff33ee85406f96a3b933d02afc2a"
            ],
            "layout": "IPY_MODEL_862214273d5a4bb0a18084b1df0e5e20"
          }
        },
        "97fcbc8d8c4b414f98b4e30b6c9ea684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23404715c68343c4afbd088afb35bad6",
            "placeholder": "​",
            "style": "IPY_MODEL_b28a9a2187ab44798ffca09e7ee3d9ee",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "52c219436a2449acb3be8ac853b6b147": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ea296b8ec53480ea2dea15d226b9d43",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_07816235fbc34188a988366be7e9c356",
            "value": 48
          }
        },
        "5fe4ff33ee85406f96a3b933d02afc2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f302e42c956435da3f022a2e121a68e",
            "placeholder": "​",
            "style": "IPY_MODEL_7fb8b9719a0247b9b54f0fc099bbe974",
            "value": " 48.0/48.0 [00:00&lt;00:00, 3.46kB/s]"
          }
        },
        "862214273d5a4bb0a18084b1df0e5e20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23404715c68343c4afbd088afb35bad6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b28a9a2187ab44798ffca09e7ee3d9ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ea296b8ec53480ea2dea15d226b9d43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07816235fbc34188a988366be7e9c356": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f302e42c956435da3f022a2e121a68e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fb8b9719a0247b9b54f0fc099bbe974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5fd9524b5fb429cadc4fd256dd347b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_60b1d60f1d1f484dab982002ae30cf61",
              "IPY_MODEL_230e34b5b9944f3185621507e82712ee",
              "IPY_MODEL_96c8495098a74ce282f93dc2212d9d20"
            ],
            "layout": "IPY_MODEL_6f96286f53b24805bb049db02cbd1c62"
          }
        },
        "60b1d60f1d1f484dab982002ae30cf61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05c156b402b14e74a2240f64d2613e3a",
            "placeholder": "​",
            "style": "IPY_MODEL_70d5b88893be4c4d8aa4d49210b11a8c",
            "value": "vocab.txt: 100%"
          }
        },
        "230e34b5b9944f3185621507e82712ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_818fd551f097473aa243351330f34a6f",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9699b2a7fc264665b58bc16d85efa120",
            "value": 231508
          }
        },
        "96c8495098a74ce282f93dc2212d9d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d17140369e1242abb3b3fd99365bb6fe",
            "placeholder": "​",
            "style": "IPY_MODEL_8ad3c77f112342bf87bdee715442833b",
            "value": " 232k/232k [00:00&lt;00:00, 1.83MB/s]"
          }
        },
        "6f96286f53b24805bb049db02cbd1c62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05c156b402b14e74a2240f64d2613e3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70d5b88893be4c4d8aa4d49210b11a8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "818fd551f097473aa243351330f34a6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9699b2a7fc264665b58bc16d85efa120": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d17140369e1242abb3b3fd99365bb6fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ad3c77f112342bf87bdee715442833b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}